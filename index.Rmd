---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Elicenda Tovar (et22536)

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information. HELPmiss:	Health Evaluation and Linkage to Primary Care. Datasets 1-4 were used in project one. All of the data was obtained from the Texas county health ranking website. The new data includes the proportion of adult individuals that are considered obese in each of the Texas counties in 2017 and the proportion of the population who are low-income and do not live close to a grocery store in 2015. The binary variable was created with the proportion of individuals in each county that have some college education from 2015-2919. I chose 49% and above to be the cut off for the 
"1" value. 

```{R}
library(tidyverse)
data1 <- read.csv("~/pmhd.csv") #poor mental health days
data2 <- read.csv("~/pphd.csv") #poor physical health days
data3 <- read.csv("~/obesity.csv") #proportion of obese individuals in each county
data4 <- read.csv("~/LAHF.csv") #proportion of individuals in each county with limited access to healthy foods.
data5 <- read.csv("~/SomeCollege.csv") # proportion of individuals with some college education 

# cleaning the detasets 
data1<- subset(data1, select=c(County,County.Value.))
data1<-rename(data1, PoorMHdays=County.Value.)
cunt1<-data1 %>% group_by(County) %>% summarise(n())

data2<- subset(data2, select=c(County,County.Value.))
data2<-rename(data2, PoorPHdays=County.Value.)
count2<-data2 %>% group_by(County) %>% summarise(n())

unique<-union(data1$County, data2$County)
unique2<-union(data3$County, data4$County)

# joining code below 
full<- full_join(data1, data2, by=c("County"))
full2<- full_join(full, data3, by=c("County"))
full3<- full_join(full2, data4, by=c("County"))
full4<- full_join(full3, data5, by=c("County"))

newfull <- mutate(full4, GoodMH = ifelse(PoorMHdays < 4.5 ,"Yes", "No"))  #made a new column with Good Mental Health days 
datafinal <- mutate(newfull, College = ifelse(SomeCollege > 0.49 ,"1", "0")) #creating a binary variable 


```

### Cluster Analysis

```{R}
library(cluster)
# clustering code here
clustdata <- datafinal[c(4:6)]
#sil_Width with PAM 
sil_width<-vector() 
for(i in 2:10){  
  kms <- pam(clustdata,k=i) 
  sil_width[i]<-kms$silinfo$avg.width
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)


clustdata<- datafinal %>% select(P.Obese, P.LAHF,SomeCollege)
set.seed(123)
pam1<- clustdata %>% pam(k=2)

#visualize
library(GGally)
pamd<-clustdata %>% mutate(cluster=as.factor(pam1$clustering))
pamd %>% ggpairs(columns = 1:3, aes(color=cluster))
```

Include a paragraph or two describing results found, interpreting the clusters in terms of the original variables and observations, discussing goodness of fit of the cluster solution, etc.

### Dimensionality Reduction with PCA

```{R}
# PCA code here
clustdatnona<-na.omit(clustdata)
pcad <- princomp(clustdatnona, cor = T)
summary(pcad, loadings=T)
pcad$scores %>% cor %>% round() #3 PCAs not correlated w each other 

pcad1 <- data.frame(PC1 = pcad$scores[,1], PC2 = pcad$scores[,2], PC3=pcad$scores[,3])
pcad1 <- pcad1 %>% mutate(SomeCollege = clustdatnona$SomeCollege)
dim(pcad1)
#pcad1

#visualize 
ggplot(pcad1, aes(PC1, PC2, PC3)) + geom_point(aes(color = SomeCollege)) + theme_dark()
```

Include a paragraph or two describing results with a focus on what it means to score high/low on each PC you retain (interpreting each PC you retained in terms of the original variables/loadings); also discuss how much of the total variance in your dataset is explained by these PCs.

Discussions of PCA here. 


###  Linear Classifier

```{R}
# linear classifier code here
nonadata<- na.omit(datafinal)
logfit <- glm(nonadata$SomeCollege ~ PoorMHdays + PoorPHdays + P.Obese + P.LAHF, data = nonadata, family = "binomial")
score <- predict(logfit, type = "response")
score %>% round(3) #%>% max()

#confusion matrix
class_diag(score, truth = nonadata$College, positive = 1)  
table(truth= nonadata$College, predictions= score>.5)

```

```{R}
# cross-validation of linear classifier here (k-fold CV)
set.seed(1234)
k=10
data<-nonadata[sample(nrow(nonadata)),] 
folds<-rep(1:k, length.out = nrow(nonadata)) 

diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$College
fit <- glm(College == "1" ~ PoorMHdays + PoorPHdays + P.Obese + P.LAHF, data = train, family = "binomial")  
  
probs<-predict(fit,newdata = test,type="response")
diags<-rbind(diags,class_diag(probs,truth, positive="1"))
}  

summarise_all(diags, mean)

```
Both models are 
Discuss the results in a paragraph. How well is your model predicting new observations per CV AUC? Do you see signs of overfitting?

Discussion here

The maximum predicted probability was 0.695. 

### Non-Parametric Classifier

```{R}
library(caret)
set.seed(1234)
# non-parametric classifier; K-nearest-neighbors 
knn_fit <- knn3(nonadata$College ~ PoorMHdays + PoorPHdays + P.Obese + P.LAHF, data = nonadata)
pred_knn <- predict(knn_fit, newdata=nonadata)[, 2]
class_diag(pred_knn, nonadata$College, positive = "1")

#confusion matrix 
table(truth= nonadata$College, predictions= pred_knn>.5)

```

```{R}
# cross-validation of np classifier here
set.seed(1234)
k = 10

data <- sample_frac(nonadata)  #randomly order rows
folds <- rep(1:k, length.out = nrow(data))  #create folds

diags <- NULL

i = 1
for (i in 1:k) {
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$College
    # train model
    fit <- knn3(College == "1" ~ PoorMHdays + PoorPHdays + P.Obese + P.LAHF, data = train)
    # test model
    probs <- predict(fit, newdata = test)[, 2]
    diags <- rbind(diags, class_diag(probs, truth, positive = "1"))
}
summarize_all(diags, mean)

```
Discuss the results in a paragraph. How well is your model predicting new observations per CV AUC? Do you see signs of overfitting? How does your nonparametric model compare with the linear model in its cross-validation performance?

Discussion


### Regression/Numeric Prediction

```{R}
# regression model code here
nonadata<- na.omit(datafinal)
logfit <- lm(nonadata$SomeCollege ~ PoorMHdays + PoorPHdays + P.Obese + P.LAHF, data = nonadata, family = "binomial")  

pred<-predict(logfit)

#MSE 
mean((nonadata$SomeCollege-pred)^2)
```

```{R}
# cross-validation of regression model here classification 4 slide 45 
set.seed(1234)
k=10
data<-nonadata[sample(nrow(nonadata)),] #randomly order rows
folds<-cut(seq(1:nrow(nonadata)),breaks=k,labels=F) #create folds
MSE<-NULL

for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  truth<-test$SomeCollege

  ## Fit linear regression model to training set
  fit<-lm(SomeCollege~ PoorMHdays + PoorPHdays + P.Obese + P.LAHF,data=train)
  yhat<-predict(fit,newdata=test)
  ## Compute prediction error  (MSE) for fold i
  MSE[i]<-mean((test$SomeCollege-yhat)^2) 
}
mean(MSE) 
```

The prediction on the linear regression has the ability to predict similarly to the cross validation training set. The means squared error is 0.009 and 0.010, respectively. This does not show any signs of overfitting since the cross validation means squared error is not much larger than the linear regression MSE. 

### Python 

```{R}
library(reticulate)
use_python("usr/bin/python3")
#py_install("matplotlib")
plot<- import("matplotlib")
plot$use("Agg", force = TRUE)

```

```{python}
# python code here
import matplotlib.pyplot as plt
import numpy as np 

x=r.datafinal['SomeCollege']
y=r.datafinal['P.Obese'] #lack of access to healthy food 
plt.scatter(x,y)
```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




